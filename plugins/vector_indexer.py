"""Vector Indexer Plugin for Scribe MCP

This plugin provides automatic vector indexing for Scribe log entries,
enabling semantic search capabilities while maintaining repository isolation.

Features:
- Background embedding generation with asyncio queue
- Repository-scoped FAISS index management
- Deterministic UUID-based entry indexing
- Graceful fallback when dependencies unavailable
- Atomic index updates with rollback capability
"""

from __future__ import annotations

import asyncio
import json
import logging
import sqlite3
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import threading

# Vector processing imports (optional)
try:
    import faiss
    import numpy as np
    from sentence_transformers import SentenceTransformer
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    faiss = None
    np = None
    SentenceTransformer = None

from scribe_mcp.plugins.registry import HookPlugin
from scribe_mcp.config.repo_config import RepoConfig
from scribe_mcp.config.settings import settings
from scribe_mcp.config.vector_config import load_vector_config
from scribe_mcp.storage.models import VectorIndexRecord, VectorShardMetadata
from scribe_mcp.utils.time import utcnow

# Setup logging
plugin_logger = logging.getLogger(__name__)


class VectorIndexer(HookPlugin):
    """Vector indexing plugin for Scribe log entries."""

    name = "vector_indexer"
    version = "1.0.0"
    description = "Automatic vector indexing for semantic search"
    author = "VectorIndexAgent"

    def __init__(self):
        self.repo_config: Optional[RepoConfig] = None
        self.repo_root: Optional[Path] = None
        self.repo_slug: Optional[str] = None
        self.vector_config: Optional[VectorConfig] = None

        # Vector processing components
        self.embedding_model: Optional[SentenceTransformer] = None
        self.vector_index: Optional[faiss.Index] = None
        self.index_metadata: Optional[VectorShardMetadata] = None

        # Background processing
        self.embedding_queue: Optional[asyncio.Queue] = None
        self.queue_worker_task: Optional[asyncio.Task] = None
        self.queue_lock: Optional[asyncio.Lock] = None
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._loop_thread: Optional[threading.Thread] = None
        self._loop_ready = threading.Event()
        self._owns_loop = False

        # Database connection for mapping
        self.mapping_db_path: Optional[Path] = None
        self._db_lock = threading.Lock()

        # State tracking
        self.initialized = False
        self.enabled = False  # Will be set from config
        self._shutdown_event = threading.Event()

    def initialize(self, config: RepoConfig) -> None:
        """Initialize the vector indexer plugin."""
        # Load configuration
        self.vector_config = load_vector_config(config.repo_root)
        self.enabled = self.vector_config.enabled

        if not self.enabled:
            plugin_logger.info("Vector indexing disabled via configuration")
            return

        if not FAISS_AVAILABLE:
            plugin_logger.warning("FAISS dependencies not available, vector indexing disabled")
            self.enabled = False
            return

        try:
            self.repo_config = config
            self.repo_root = config.repo_root
            self.repo_slug = config.repo_slug or self._get_repo_slug(self.repo_root)

            # Initialize vector components
            self._init_embedding_model()
            self._init_vector_index()
            self._init_mapping_database()
            self._init_background_queue()

            self.initialized = True
            plugin_logger.info(f"VectorIndexer initialized for repository: {self.repo_slug}")

        except Exception as e:
            plugin_logger.error(f"Failed to initialize VectorIndexer: {e}", exc_info=True)
            self.enabled = False

    def cleanup(self) -> None:
        """Cleanup plugin resources."""
        if not self.initialized:
            return

        try:
            # Signal shutdown
            self._shutdown_event.set()

            # Cancel background worker
            if self.queue_worker_task:
                if self._owns_loop and self._loop:
                    def _cancel_task() -> None:
                        if not self.queue_worker_task.done():
                            self.queue_worker_task.cancel()

                    self._loop.call_soon_threadsafe(_cancel_task)
                elif not self.queue_worker_task.done():
                    self.queue_worker_task.cancel()

            # Stop dedicated event loop if we own it
            if self._owns_loop and self._loop:
                self._loop.call_soon_threadsafe(self._loop.stop)
                if self._loop_thread and self._loop_thread.is_alive():
                    self._loop_thread.join(timeout=5)
                self._loop = None
                self._loop_thread = None
                self.embedding_queue = None
                self.queue_lock = None
                self._owns_loop = False

            # Close database connection
            with self._db_lock:
                if hasattr(self, '_db_conn') and self._db_conn:
                    self._db_conn.close()

            plugin_logger.info("VectorIndexer cleanup completed")

        except Exception as e:
            plugin_logger.error(f"Error during VectorIndexer cleanup: {e}")

    def post_append(self, entry_data: Dict[str, Any]) -> None:
        """Called after an entry is appended - queue for vector indexing."""
        if not self.initialized or not self.enabled:
            plugin_logger.debug("Vector indexer inactive, skipping entry")
            return

        if self._shutdown_event.is_set():
            plugin_logger.warning("Vector indexer shutting down, skipping entry")
            return

        if not self.embedding_queue or not self._loop:
            plugin_logger.warning("Embedding queue not available, skipping entry")
            return

        try:
            future = asyncio.run_coroutine_threadsafe(
                self._queue_entry_for_embedding(entry_data),
                self._loop
            )
            future.add_done_callback(self._log_async_error)
        except Exception as e:
            plugin_logger.error(f"Failed to queue entry for vector indexing: {e}")

    @staticmethod
    def _log_async_error(future: asyncio.Future) -> None:
        """Log exceptions from background scheduling."""
        try:
            future.result()
        except Exception as exc:
            plugin_logger.error(f"Vector queue scheduling error: {exc}")

    def _get_repo_slug(self, repo_root: Path) -> str:
        """Generate repository slug from root path."""
        import re
        repo_name = repo_root.name
        slug = re.sub(r'[^a-zA-Z0-9_-]', '-', repo_name.lower())
        slug = re.sub(r'-+', '-', slug).strip('-')
        return slug or "unknown-repo"

    def _init_embedding_model(self) -> None:
        """Initialize the sentence transformer model."""
        try:
            model_name = self.vector_config.model
            self.embedding_model = SentenceTransformer(model_name)
            plugin_logger.info(f"Loaded embedding model: {model_name}")
        except Exception as e:
            plugin_logger.error(f"Failed to load embedding model: {e}")
            raise

    def _init_vector_index(self) -> None:
        """Initialize or load the FAISS index."""
        try:
            vectors_dir = self.repo_root / ".scribe_vectors"
            vectors_dir.mkdir(exist_ok=True)

            index_path = vectors_dir / f"{self.repo_slug}.faiss"
            metadata_path = vectors_dir / f"{self.repo_slug}.meta.json"

            # Load or create index
            if index_path.exists() and metadata_path.exists():
                self._load_existing_index(index_path, metadata_path)
            else:
                self._create_new_index(index_path, metadata_path)

            plugin_logger.info(f"Vector index ready: {self.index_metadata.total_entries} entries")

        except Exception as e:
            plugin_logger.error(f"Failed to initialize vector index: {e}")
            raise

    def _load_existing_index(self, index_path: Path, metadata_path: Path) -> None:
        """Load existing FAISS index and metadata."""
        # Load metadata
        with open(metadata_path, 'r') as f:
            metadata_dict = json.load(f)

        self.index_metadata = VectorShardMetadata(
            repo_slug=metadata_dict['repo_slug'],
            dimension=metadata_dict['dimension'],
            model=metadata_dict['model'],
            scope=metadata_dict['scope'],
            created_at=datetime.fromisoformat(metadata_dict['created_at']),
            backend=metadata_dict['backend'],
            index_type=metadata_dict['index_type'],
            total_entries=metadata_dict['total_entries'],
            last_updated=datetime.fromisoformat(metadata_dict['last_updated']) if metadata_dict.get('last_updated') else None,
            embedding_model_version=metadata_dict.get('embedding_model_version'),
            index_size_bytes=index_path.stat().st_size if index_path.exists() else None
        )

        # Validate configuration compatibility
        if (self.index_metadata.dimension != self.vector_config.dimension or
            self.index_metadata.model != self.vector_config.model):
            plugin_logger.warning(
                f"Index configuration mismatch. Index: dim={self.index_metadata.dimension}, "
                f"model={self.index_metadata.model}. Settings: dim={self.vector_config.dimension}, "
                f"model={self.vector_config.model}. Consider rebuilding index."
            )

        # Load FAISS index
        self.vector_index = faiss.read_index(str(index_path))

        # Load GPU if enabled
        if self.vector_config.gpu and hasattr(faiss, 'StandardGpuResources'):
            try:
                res = faiss.StandardGpuResources()
                self.vector_index = faiss.index_cpu_to_gpu(res, 0, self.vector_index)
                plugin_logger.info("GPU acceleration enabled for vector index")
            except Exception as e:
                plugin_logger.warning(f"Failed to enable GPU acceleration: {e}")

    def _create_new_index(self, index_path: Path, metadata_path: Path) -> None:
        """Create new FAISS index and metadata."""
        dimension = self.vector_config.dimension

        # Create FAISS index (IndexFlatIP for inner product)
        self.vector_index = faiss.IndexFlatIP(dimension)

        # Create metadata
        self.index_metadata = VectorShardMetadata(
            repo_slug=self.repo_slug,
            dimension=dimension,
            model=self.vector_config.model,
            scope="repo-local",
            created_at=utcnow(),
            backend="faiss",
            index_type="IndexFlatIP",
            total_entries=0,
            last_updated=utcnow(),
            embedding_model_version=getattr(self.embedding_model, 'version', 'unknown')
        )

        # Save initial metadata
        self._save_index_metadata(metadata_path)
        plugin_logger.info(f"Created new vector index: {dimension}D")

    def _init_mapping_database(self) -> None:
        """Initialize the SQLite database for UUID mapping."""
        vectors_dir = self.repo_root / ".scribe_vectors"
        self.mapping_db_path = vectors_dir / "mapping.sqlite"

        with self._db_lock:
            self._db_conn = sqlite3.connect(str(self.mapping_db_path), check_same_thread=False)
            self._db_conn.row_factory = sqlite3.Row

            # Create tables
            self._db_conn.execute("""
                CREATE TABLE IF NOT EXISTS vector_entries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    entry_id TEXT UNIQUE NOT NULL,
                    project_slug TEXT NOT NULL,
                    repo_slug TEXT NOT NULL,
                    vector_rowid INTEGER NOT NULL,
                    text_content TEXT NOT NULL,
                    agent_name TEXT,
                    timestamp_utc TEXT NOT NULL,
                    metadata_json TEXT,
                    embedding_model TEXT,
                    vector_dimension INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Create indexes
            self._db_conn.execute("CREATE INDEX IF NOT EXISTS idx_entry_id ON vector_entries(entry_id)")
            self._db_conn.execute("CREATE INDEX IF NOT EXISTS idx_project_slug ON vector_entries(project_slug)")
            self._db_conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON vector_entries(timestamp_utc)")

            self._db_conn.commit()

    def _init_background_queue(self) -> None:
        """Initialize the background embedding processing queue."""
        maxsize = self.vector_config.queue_max

        plugin_logger.info("Starting dedicated background loop for vector processing")
        self._owns_loop = True
        self._loop = asyncio.new_event_loop()
        self._loop_ready.clear()

        self._loop_thread = threading.Thread(
            target=self._start_background_loop,
            args=(maxsize,),
            name=f"VectorIndexerLoop-{self.repo_slug}",
            daemon=True
        )
        self._loop_thread.start()

        if not self._loop_ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for vector background loop to start")

        plugin_logger.info(f"Background queue initialized on dedicated loop (maxsize: {maxsize})")

    def _start_background_loop(self, maxsize: int) -> None:
        """Run a dedicated asyncio loop for vector processing."""
        if not self._loop:
            return

        asyncio.set_event_loop(self._loop)
        self.embedding_queue = asyncio.Queue(maxsize=maxsize)
        self.queue_lock = asyncio.Lock()
        self.queue_worker_task = self._loop.create_task(self._queue_worker())
        self._loop_ready.set()

        try:
            self._loop.run_forever()
        finally:
            tasks = asyncio.all_tasks(self._loop)
            for task in tasks:
                task.cancel()
            if tasks:
                self._loop.run_until_complete(asyncio.gather(*tasks, return_exceptions=True))
            self._loop.run_until_complete(self._loop.shutdown_asyncgens())
            self._loop.close()
            asyncio.set_event_loop(None)

    async def _queue_entry_for_embedding(self, entry_data: Dict[str, Any]) -> None:
        """Queue an entry for background embedding processing."""
        if not self.embedding_queue:
            return

        try:
            # Prepare entry data for embedding
            embedding_task = {
                'entry_id': entry_data.get('entry_id'),
                'project_slug': entry_data.get('project_name', '').lower().replace(' ', '-'),
                'text_content': entry_data.get('message', ''),
                'agent_name': entry_data.get('agent', ''),
                'timestamp_utc': entry_data.get('timestamp', ''),
                'metadata_json': json.dumps(entry_data.get('meta', {})),
                'embedding_model': self.vector_config.model,
                'vector_dimension': self.vector_config.dimension,
                'retry_count': 0,
                'queued_at': utcnow()
            }

            # Add to queue (non-blocking)
            self.embedding_queue.put_nowait(embedding_task)

        except asyncio.QueueFull:
            plugin_logger.warning(f"Vector embedding queue full, dropping entry: {entry_data.get('entry_id')}")
        except Exception as e:
            plugin_logger.error(f"Failed to queue entry for embedding: {e}")

    async def _queue_worker(self) -> None:
        """Background worker for processing embedding queue."""
        batch_size = self.vector_config.batch_size
        plugin_logger.info(f"Vector embedding worker started (batch_size: {batch_size})")

        while not self._shutdown_event.is_set():
            try:
                # Collect batch of entries
                batch = []
                timeout = 1.0  # Wait up to 1 second for first item

                while len(batch) < batch_size and not self._shutdown_event.is_set():
                    try:
                        item = await asyncio.wait_for(self.embedding_queue.get(), timeout=timeout)
                        batch.append(item)
                        timeout = 0.1  # Short timeout for subsequent items
                    except asyncio.TimeoutError:
                        if batch:  # Process if we have items
                            break
                        continue  # Continue waiting for first item

                if batch and not self._shutdown_event.is_set():
                    await self._process_embedding_batch(batch)

            except Exception as e:
                plugin_logger.error(f"Error in embedding queue worker: {e}")
                await asyncio.sleep(1)  # Brief pause before retry

        plugin_logger.info("Vector embedding worker stopped")

    async def _process_embedding_batch(self, batch: List[Dict[str, Any]]) -> None:
        """Process a batch of entries for embedding."""
        try:
            # Extract texts for batch embedding
            texts = [item['text_content'] for item in batch]

            # Generate embeddings
            embeddings = self.embedding_model.encode(
                texts,
                batch_size=len(texts),
                convert_to_numpy=True
            )

            # Normalize embeddings for cosine similarity
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

            # Store embeddings and mapping data
            await self._store_embeddings_batch(batch, embeddings)

            plugin_logger.debug(f"Processed embedding batch: {len(batch)} entries")

        except Exception as e:
            plugin_logger.error(f"Failed to process embedding batch: {e}")

            # Retry failed items
            for item in batch:
                item['retry_count'] += 1
                if item['retry_count'] < 3:  # Max 3 retries
                    await asyncio.sleep(2 ** item['retry_count'])  # Exponential backoff
                    await self._queue_entry_for_embedding(item)

    async def _store_embeddings_batch(self, batch: List[Dict[str, Any]], embeddings: np.ndarray) -> None:
        """Store embeddings and update mapping database."""
        if not self.queue_lock:
            raise RuntimeError("Queue lock not initialized")

        async with self.queue_lock:
            try:
                start_rowid = self.vector_index.ntotal

                # Add embeddings to FAISS index
                self.vector_index.add(embeddings)

                # Save FAISS index
                vectors_dir = self.repo_root / ".scribe_vectors"
                index_path = vectors_dir / f"{self.repo_slug}.faiss"
                faiss.write_index(self.vector_index, str(index_path))

                # Update mapping database
                with self._db_lock:
                    for i, item in enumerate(batch):
                        vector_rowid = start_rowid + i

                        self._db_conn.execute("""
                            INSERT OR REPLACE INTO vector_entries
                            (entry_id, project_slug, repo_slug, vector_rowid, text_content,
                             agent_name, timestamp_utc, metadata_json, embedding_model, vector_dimension)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            item['entry_id'],
                            item['project_slug'],
                            self.repo_slug,
                            vector_rowid,
                            item['text_content'],
                            item['agent_name'],
                            item['timestamp_utc'],
                            item['metadata_json'],
                            item['embedding_model'],
                            item['vector_dimension']
                        ))

                    self._db_conn.commit()

                # Update metadata
                self.index_metadata.total_entries = self.vector_index.ntotal
                self.index_metadata.last_updated = utcnow()
                self.index_metadata.index_size_bytes = index_path.stat().st_size

                metadata_path = vectors_dir / f"{self.repo_slug}.meta.json"
                self._save_index_metadata(metadata_path)

                plugin_logger.debug(f"Stored {len(batch)} embeddings, total: {self.index_metadata.total_entries}")

            except Exception as e:
                plugin_logger.error(f"Failed to store embeddings batch: {e}")
                raise

    def _save_index_metadata(self, metadata_path: Path) -> None:
        """Save index metadata to JSON file."""
        metadata_dict = {
            'repo_slug': self.index_metadata.repo_slug,
            'dimension': self.index_metadata.dimension,
            'model': self.index_metadata.model,
            'scope': self.index_metadata.scope,
            'created_at': self.index_metadata.created_at.isoformat(),
            'backend': self.index_metadata.backend,
            'index_type': self.index_metadata.index_type,
            'total_entries': self.index_metadata.total_entries,
            'last_updated': self.index_metadata.last_updated.isoformat() if self.index_metadata.last_updated else None,
            'embedding_model_version': self.index_metadata.embedding_model_version
        }

        # Atomic write
        temp_path = metadata_path.with_suffix('.json.tmp')
        with open(temp_path, 'w') as f:
            json.dump(metadata_dict, f, indent=2)
        temp_path.rename(metadata_path)

    # Public API methods for MCP tools
    def get_index_status(self) -> Dict[str, Any]:
        """Get current index status and statistics."""
        if not self.initialized:
            return {
                'initialized': False,
                'enabled': self.enabled,
                'error': 'Plugin not initialized'
            }

        return {
            'initialized': True,
            'enabled': self.enabled,
            'repo_slug': self.repo_slug,
            'model': self.vector_config.model,
            'dimension': self.vector_config.dimension,
            'total_entries': self.index_metadata.total_entries if self.index_metadata else 0,
            'last_updated': self.index_metadata.last_updated.isoformat() if self.index_metadata and self.index_metadata.last_updated else None,
            'queue_depth': self.embedding_queue.qsize() if self.embedding_queue else 0,
            'queue_max': self.vector_config.queue_max,
            'gpu_enabled': self.vector_config.gpu,
            'faiss_available': FAISS_AVAILABLE
        }

    def search_similar(self, query: str, k: int = 10, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Search for similar entries using vector similarity."""
        if not self.initialized or not self.vector_index or not self.embedding_model:
            return []

        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode([query])
            query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)

            # Search in FAISS index
            k = min(k, self.vector_index.ntotal)
            if k == 0:
                return []

            distances, rowids = self.vector_index.search(query_embedding, k)

            # Retrieve entry data from mapping database
            results = []
            with self._db_lock:
                for i, rowid in enumerate(rowids[0]):
                    distance = float(distances[0][i])

                    # Get entry data
                    cursor = self._db_conn.execute("""
                        SELECT entry_id, project_slug, text_content, agent_name,
                               timestamp_utc, metadata_json
                        FROM vector_entries
                        WHERE vector_rowid = ? AND repo_slug = ?
                    """, (int(rowid), self.repo_slug))

                    row = cursor.fetchone()
                    if row:
                        # Apply filters if provided
                        if filters:
                            if not self._apply_filters(row, filters):
                                continue

                        result = {
                            'entry_id': row['entry_id'],
                            'project_slug': row['project_slug'],
                            'text_content': row['text_content'],
                            'agent_name': row['agent_name'],
                            'timestamp_utc': row['timestamp_utc'],
                            'metadata_json': row['metadata_json'],
                            'similarity_score': distance,
                            'vector_rowid': int(rowid)
                        }
                        results.append(result)

            return results

        except Exception as e:
            plugin_logger.error(f"Vector search failed: {e}")
            return []

    def retrieve_by_uuid(self, entry_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve entry by UUID."""
        if not self.initialized:
            return None

        try:
            with self._db_lock:
                cursor = self._db_conn.execute("""
                    SELECT entry_id, project_slug, text_content, agent_name,
                           timestamp_utc, metadata_json, vector_rowid
                    FROM vector_entries
                    WHERE entry_id = ? AND repo_slug = ?
                """, (entry_id, self.repo_slug))

                row = cursor.fetchone()
                if row:
                    return {
                        'entry_id': row['entry_id'],
                        'project_slug': row['project_slug'],
                        'text_content': row['text_content'],
                        'agent_name': row['agent_name'],
                        'timestamp_utc': row['timestamp_utc'],
                        'metadata_json': row['metadata_json'],
                        'vector_rowid': row['vector_rowid']
                    }

            return None

        except Exception as e:
            plugin_logger.error(f"Failed to retrieve entry by UUID: {e}")
            return None

    def _apply_filters(self, row: sqlite3.Row, filters: Dict[str, Any]) -> bool:
        """Apply filters to search results."""
        try:
            # Project filter
            if 'project_slug' in filters:
                if row['project_slug'] != filters['project_slug']:
                    return False

            # Time range filter
            if 'time_range' in filters:
                time_range = filters['time_range']
                if isinstance(time_range, dict):
                    start = time_range.get('start')
                    end = time_range.get('end')

                    timestamp = datetime.fromisoformat(row['timestamp_utc'].replace(' UTC', ''))

                    if start and timestamp < datetime.fromisoformat(start):
                        return False
                    if end and timestamp > datetime.fromisoformat(end):
                        return False

            # Agent filter
            if 'agent_name' in filters:
                if row['agent_name'] != filters['agent_name']:
                    return False

            return True

        except Exception as e:
            plugin_logger.warning(f"Filter application failed: {e}")
            return True  # Default to include if filtering fails

    def rebuild_index(self) -> Dict[str, Any]:
        """Rebuild the entire vector index from scratch."""
        if not self.initialized or not self.enabled:
            raise RuntimeError("Vector indexer not initialized or enabled")

        plugin_logger.info("Starting vector index rebuild")
        rebuild_start = time.time()

        try:
            # Stop background processing
            self._stop_background_processing()

            # Get old statistics
            old_entries = self.index_metadata.total_entries if self.index_metadata else 0

            # Clear existing index
            self.vector_index.reset()
            if self._db_conn:
                with self._db_lock:
                    self._db_conn.execute("DELETE FROM vector_entries WHERE repo_slug = ?", (self.repo_slug,))
                    self._db_conn.commit()

            # Reset metadata
            if self.index_metadata:
                self.index_metadata.total_entries = 0
                self.index_metadata.last_updated = utcnow()

            # Restart background processing
            self._start_background_processing()

            rebuild_time = time.time() - rebuild_start

            plugin_logger.info(f"Vector index rebuilt successfully in {rebuild_time:.2f}s")

            return {
                "success": True,
                "rebuild_time_seconds": round(rebuild_time, 2),
                "old_entries_count": old_entries,
                "new_entries_count": 0,
                "message": "Index cleared and ready for new entries"
            }

        except Exception as e:
            plugin_logger.error(f"Failed to rebuild vector index: {e}")
            raise RuntimeError(f"Vector index rebuild failed: {str(e)}") from e

    def _stop_background_processing(self) -> None:
        """Stop background queue processing."""
        if self.queue_worker_task and not self.queue_worker_task.done():
            self.queue_worker_task.cancel()
            try:
                # Wait for task to finish
                import asyncio
                loop = asyncio.get_event_loop()
                loop.run_until_complete(self.queue_worker_task)
            except (asyncio.CancelledError, RuntimeError):
                pass

        plugin_logger.info("Background processing stopped")

    def _start_background_processing(self) -> None:
        """Start background queue processing if not already running."""
        if not self.queue_worker_task or self.queue_worker_task.done():
            try:
                import asyncio
                loop = asyncio.get_event_loop()
                self.queue_worker_task = loop.create_task(self._queue_worker())
                plugin_logger.info("Background processing started")
            except RuntimeError:
                plugin_logger.warning("No event loop available, background processing disabled")
